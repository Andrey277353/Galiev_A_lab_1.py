# %% [markdown]
# 
# # Обработка данных — лабораторная работа №3  
# **Автор:** Андрей Галиев  
# **Формат:**  → `Галиев_A_lab_3.ipynb`
# 
# Этапы:
# 1. Открытие файла
# 2. Обзор и проверка разделителя/типов
# 3. Очистка пропусков и дубликатов
# 4. Преобразование типов
# 5. Кодирование категориальных признаков (бинарные: `0` — male, `1` — female)
# 6. Масштабирование (при необходимости)
# 7. Корреляция числовых признаков
# 8. Сохранение «чистой» версии датасета
# 

# %%

# 1) Импорт
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
pd.set_option("display.max_columns", 100)
pd.set_option("display.width", 120)


# %%

# 2) Открытие с авто-определением разделителя
# 2.1) Поиск и открытие файла DB_3.csv
import os
from pathlib import Path
import pandas as pd

# Возможные места хранения файла
candidates = [
    Path("DB_3.csv"),  # рядом с ноутбуком
    Path.cwd() / "DB_3.csv",
    Path.home() / "Downloads" / "DB_3.csv",
    Path.home() / "Desktop" / "DB_3.csv",
    Path(r"C:\Users\Andrey\Downloads\DB_3.csv"),  # мой случай
]

path = None
for p in candidates:
    if p.exists():
        path = p
        break

if path is None:
    raise FileNotFoundError(
        "Файл DB_3.csv не найден. "
        "Положи его рядом с ноутбуком или укажи полный путь в переменной `path`."
    )

print("Использую файл:", path)
df = pd.read_csv(path, sep=None, engine="python")
print("Размеры исходного датасета:", df.shape)
df.head()
df = pd.read_csv(path, sep=None, engine='python')
print("Размеры исходного датасета:", df.shape)
df.head()


# %%

# 3) Обзор: столбцы/типы/NaN/дубликаты
print("Столбцы:", list(df.columns))
print("\nТипы данных:")
print(df.dtypes)
print("\nNaN по столбцам:")
print(df.isna().sum())
print("\nДубликаты строк:", df.duplicated().sum())


# %%

# 4) Стандартизация названий столбцов
df.columns = [str(c).strip() for c in df.columns]
df.columns = [c.replace("\n", " ").replace("\r", " ") for c in df.columns]
df.columns = ["_".join(c.split()) for c in df.columns]
df.head(1)


# %%

# 5) Удаление дубликатов
before = len(df)
df = df.drop_duplicates().reset_index(drop=True)
print(f"Удалено дубликатов: {before - len(df)}; текущий размер: {df.shape}")


# %%

# 6) Преобразование потенциально числовых колонок
for col in df.columns:
    try_series = pd.to_numeric(df[col], errors="coerce")
    na_ratio_before = df[col].isna().mean() if hasattr(df[col], "isna") else 0
    na_ratio_after  = try_series.isna().mean()
    if na_ratio_after <= na_ratio_before + 0.05 and try_series.notna().sum() > 0:
        df[col] = try_series
print(df.dtypes.head(20))


# %%

# 7) Кодирование бинарных признаков (пример: пол)
gender_like = [c for c in df.columns if c.lower() in {"gender","sex","пол"}]
mapping_known = {
    "male": 0, "м": 0, "m": 0, "man": 0, "муж": 0, "мужчина": 0,
    "female": 1, "ж": 1, "f": 1, "woman": 1, "жен": 1, "женщина": 1,
}
for col in gender_like:
    if df[col].dtype == "O":
        df[col] = df[col].astype(str).str.strip().str.lower().map(mapping_known)
    uniques = [u for u in pd.unique(df[col]) if pd.notna(u)]
    if len(uniques) == 2 and set(uniques) <= {0,1}:
        pass
    elif len(uniques) == 2:
        u0, u1 = uniques[0], uniques[1]
        df[col] = df[col].map({u0:0, u1:1})
for col in gender_like:
    print(col, "→ уникальные:", pd.unique(df[col]))


# %%

# 8) One-Hot Encoding для низкой кардинальности
max_cardinality = 30
object_cols = [c for c in df.columns if df[c].dtype == "O"]
low_card_cols = [c for c in object_cols if df[c].nunique(dropna=True) <= max_cardinality]
print("Категориальные столбцы для OHE:", low_card_cols)
df = pd.get_dummies(df, columns=low_card_cols, dummy_na=False, drop_first=True)
print("Размер после OHE:", df.shape)


# %%

# 9) Обработка пропусков (числовые — медианой)
num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
for col in num_cols:
    if df[col].isna().any():
        df[col] = df[col].fillna(df[col].median())
print("Осталось пропусков:", int(df.isna().sum().sum()))


# %%

# 10) СМЕШАННАЯ КОРРЕЛЯЦИЯ: числовые + категориальные
import numpy as np
import pandas as pd
import math

def is_numeric(s: pd.Series) -> bool:
    return pd.api.types.is_numeric_dtype(s)

def cramers_v(x: pd.Series, y: pd.Series) -> float:
    """
    Cramér's V с коррекцией смещения.
    Для пар категориальных признаков. Возвращает значение 0..1.
    """
    # Контингенционная таблица
    tbl = pd.crosstab(x, y).to_numpy(dtype=float)
    n = tbl.sum()
    if n == 0 or tbl.size == 0:
        return np.nan

    # Ожидаемые частоты и χ^2 (без scipy)
    row_sums = tbl.sum(axis=1, keepdims=True)
    col_sums = tbl.sum(axis=0, keepdims=True)
    expected = (row_sums @ col_sums) / n
    with np.errstate(divide='ignore', invalid='ignore'):
        chi2 = np.nansum((tbl - expected) ** 2 / np.where(expected == 0, np.nan, expected))

    r, k = tbl.shape
    phi2 = chi2 / n
    # Смещение (Bergsma, Wicher 2013)
    phi2corr = max(0.0, phi2 - (k - 1) * (r - 1) / max(n - 1, 1))
    rcorr = r - (r - 1) ** 2 / max(n - 1, 1)
    kcorr = k - (k - 1) ** 2 / max(n - 1, 1)
    denom = max(min(kcorr - 1, rcorr - 1), 1e-12)
    return float(np.sqrt(phi2corr / denom))

def correlation_ratio(categories: pd.Series, measurements: pd.Series) -> float:
    """
    Корреляционное отношение η (категориальный ↔ числовой). Возвращает 0..1.
    """
    # Факторизуем категории
    cats, _ = pd.factorize(categories, sort=True)
    cats = np.asarray(cats)
    y = pd.to_numeric(measurements, errors="coerce").to_numpy()

    mask = ~np.isnan(y) & (cats >= 0)
    if mask.sum() == 0:
        return np.nan

    cats = cats[mask]
    y = y[mask]

    y_mean = np.nanmean(y)
    # Межгрупповая сумма квадратов
    ss_between = 0.0
    for c in np.unique(cats):
        grp = y[cats == c]
        if grp.size > 0:
            ss_between += grp.size * (np.nanmean(grp) - y_mean) ** 2
    # Общая сумма квадратов
    ss_total = np.nansum((y - y_mean) ** 2)
    if ss_total == 0:
        return 0.0
    return float(np.sqrt(ss_between / ss_total))

# Список столбцов и заготовка матрицы
cols = list(df.columns)
mixed_corr = pd.DataFrame(np.nan, index=cols, columns=cols)

for i, ci in enumerate(cols):
    for j, cj in enumerate(cols):
        si, sj = df[ci], df[cj]
        if is_numeric(si) and is_numeric(sj):
            mixed_corr.loc[ci, cj] = si.corr(sj)  # Pearson
        elif (not is_numeric(si)) and (not is_numeric(sj)):
            mixed_corr.loc[ci, cj] = cramers_v(si, sj)  # Cramér's V
        else:
            # один числовой, другой категориальный → η
            if is_numeric(si):
                mixed_corr.loc[ci, cj] = correlation_ratio(sj, si)
            else:
                mixed_corr.loc[ci, cj] = correlation_ratio(si, sj)

print("Размер смешанной матрицы корреляций:", mixed_corr.shape)
mixed_corr


# %%

# Визуализация корреляции (без явной настройки цветов)
import matplotlib.pyplot as plt

plt.figure(figsize=(9, 7))
plt.imshow(mixed_corr.to_numpy(dtype=float), interpolation='nearest')
plt.title("Смешанная корреляция: Pearson / Cramér's V / η")
plt.xticks(range(len(mixed_corr.columns)), mixed_corr.columns, rotation=90)
plt.yticks(range(len(mixed_corr.index)), mixed_corr.index)
plt.colorbar()
plt.tight_layout()
plt.show()


# %%

# 11) Сохранение очищенного датасета
out_path = "DB_3_clean.csv"
df.to_csv(out_path, index=False)
print("Сохранено:", out_path, "размер:", df.shape)
